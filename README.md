# Multi-modal Large Language Model Collection ðŸ¦•
This is a curated list of Multi-modal Large Language Models (MLLM), Multimodal Benchmarks (MMB), Multimodal Instruction Tuning (MMIT), Multimodal In-context Learning (MMIL), Foundation Models (*e.g.*, CLIP families) (FM), and the most popular Parameter-Efficient Tuning methods.

## ðŸ“’Table of Contents
- [Alignment](#alignment)
- [Multi-modal Large Language Models (MLLM)](#multimodal-large-language-models)
- [Multi-modal Benchmarks (MMB)](#multimodal-benchmarks)
- [Foundation Models (FM)](#foundation-models)
- [Parameter-Efficient Tuning Repo (PETR)](#parameter-efficient-tuning-repo)

> ### Alignment

* **MDPO: Conditional Preference Optimization for Multimodal Large Language Models** [arXiv 2024/06/17] [[Paper](https://arxiv.org/pdf/2406.11839)] <br>
University of Southern California, University of California, Davis, Microsoft Research

* **RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback** [CVPR 2024] [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_RLHF-V_Towards_Trustworthy_MLLMs_via_Behavior_Alignment_from_Fine-grained_Correctional_CVPR_2024_paper.pdf)] [[Code](https://github.com/RLHF-V/RLHF-V)] [[Homepage](https://rlhf-v.github.io/)]<br>
Tsinghua University, National University of Singapore, Shenzhen International Graduate School, Tsinghua University, Pengcheng Laboratory, Shenzhen, China


> ### Multi-modal Large Language Models (MLLM)

* **ShareGPT4V: Improving Large Multi-Modal Models with Better Captions** [ECCV2024] [[Paper](https://arXiv.org/abs/2311.12793)] [[Code](https://github.com/ShareGPT4Omni/ShareGPT4V)] [[Homepage](https://sharegpt4v.github.io/)]<br>
University of Science and Technology of China, Shanghai AI Laboratory

* **Video-LLaVA: Learning United Visual Representation by Alignment Before Projection** [arXiv 2024/02/12] [[Paper](https://arXiv.org/pdf/2311.10122.pdf)] [[Code](https://github.com/PKU-YuanGroup/Video-LLaVA)] <br>
Peking University, Peng Cheng Laboratory, Sun Yat-sen University, Guangzhou, Tencent Data Platform, AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, FarReel Ai Lab

* **Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models** [arXiv 2024/02/12] [[Paper](https://arXiv.org/abs/2402.07865)] [[Code](https://github.com/TRI-ML/prismatic-vlms)] [[Evaluation](https://github.com/TRI-ML/vlm-evaluation)]<br>
Stanford, Toyota Research Institute


* **Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models** [arXiv 2024/03/27] [[Paper](https://arXiv.org/pdf/2403.18814.pdf)] [[Code](https://github.com/dvlab-research/MiniGemini)] [[Project Page](https://mini-gemini.github.io/)]<br>
The Chinese University of Hong Kong, SmartMore


* **InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks** [arXiv 2024/01/15] [[Paper](https://arXiv.org/abs/2312.14238)] [[Code](https://github.com/OpenGVLab/InternVL)]<br>
OpenGVLab, Shanghai AI Laboratory, Nanjing University, The University of Hong Kong, The Chinese University of Hong Kong, Tsinghua University, University of Science and Technology of China, SenseTime Research

* **GiT: Towards Generalist Vision Transformer through Universal Language Interface** [arXiv 2024/03/14] [[Paper](https://arXiv.org/abs/2403.09394)]<br>
Peking University, Max Planck Institute for Informatics, The Chinese University of Hong Kong Shenzhen, ETH Zurich, The Chinese University of Hong Kong<br>

* **LLaMA: Open and Efficient Foundation Language Models** [arXiv 2023] [[Paper](https://arXiv.org/pdf/2302.13971v1.pdf)] [[Github Repo](https://github.com/CHENGY12/PLOT)]<br>
Meta AI

> ## Multimodal Benchmarks (MMB)

* **MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs** [arXiv 2024/06/17] [[Paper](https://arXiv.org/pdf/2406.11833)] [[Code](https://github.com/Liuziyu77/MMDU)] [[HomePage](https://liuziyu77.github.io/MMDU/)] [[SpaceðŸ¤—](https://huggingface.co/datasets/laolao77/MMDU)]<br>
Wuhan University, Shanghai AI Laboratory, The Chinese University of Hong Kong, MThreads, Inc.

> ### Foundation Models (FM)

> ### Parameter-Efficient Tuning Repo (PETR)
* **PEFT: Parameter-Efficient Fine-Tuning** [HuggingFace ðŸ¤—] [[Home Page](https://huggingface.co/docs/peft/index)] [[Code](https://github.com/huggingface/peft)]<br>
PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. <br>

* **LLaMA Efficient Tuning** [[Github Repo](https://github.com/hiyouga/LLaMA-Efficient-Tuning)]<br>
Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA) (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen). <br>

* **LLaMA-Adapter: Efficient Fine-tuning of LLaMA** ðŸš€[[Code](https://github.com/OpenGVLab/LLaMA-Adapter)]<br>
Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters <br>

* **LLaMA2-Accessory** ðŸš€[[Code](https://github.com/Alpha-VLLM/LLaMA2-Accessory)]<br>
An Open-source Toolkit for LLM Development <be>

* **LLaMA Factory: Training and Evaluating Large Language Models with Minimal Effort** [Code](https://github.com/hiyouga/LLaMA-Factory)]<br>
Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM3)
