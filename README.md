# Multi-modal Large Language Model Collection ðŸ¦•
This is a curated list of Multi-modal Large Language Models (MLLM), Multimodal Benchmarks (MMB), Multimodal Instruction Tuning (MMIT), Multimodal In-context Learning (MMIL), Foundation Models (*e.g.*, CLIP families) (FM), and the most popular Parameter-Efficient Tuning methods.

## ðŸ“’Table of Contents
- [Multi-modal Large Language Models (MLLM)](#multimodal-large-language-models)
- [Multimodal Benchmarks (MMB)](#multimodal-benchmarks)
- [Multimodal Instruction Tuning (MMIT)](#multimodal-instruction-tuning)
- [Multimodal In-context Learning (MMIL)](#multimodal-in-context-learning)
- [Prompt Learning (PL)](#prompt-learning)
- [Foundation Models (FM)](#foundation-models)
- [Parameter-Efficient Tuning (PET)](#parameter-efficient-tuning)

> ### Multi-modal Large Language Models (MLLM)

* **LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark** [Arxiv 2023] [[paper](https://arxiv.org/pdf/2306.06687.pdf)]<br>
<sup>1</sup>Shanghai AI Lab, <sup>2</sup>Beihang University, <sup>3</sup>The Chinese University of Hong Kong (Shenzhen), <sup>4</sup>Fudan University, <sup>5</sup>Dalian University of Technology, <sup>6</sup>The University of Sydney<br>

> ### Foundation Models (FM)
* **LLaMA: Open and Efficient Foundation Language Models** [Arxiv 2023] [[paper](https://arxiv.org/pdf/2302.13971v1.pdf)] [[Github Repo](https://github.com/CHENGY12/PLOT)]<br>
<sup>1</sup>Meta AI

> ### Prompt Learning (PL)
* **PLOT: Prompt Learning with Optimal Transport for Vision-Language Models** [ICLR 2023] [[paper](https://arxiv.org/pdf/2210.01253.pdf)] [[Github Repo](https://github.com/facebookresearch/llama)]<br>
<sup>1</sup>CMU, <sup>2</sup>Mohamed bin Zayed University of Artificial Intelligence, <sup>3</sup>Tsinghua University, <sup>4</sup>New York University<br>

> ### Parameter-Efficient Tuning (PET)
* **PEFT: Parameter-Efficient Fine-Tuning** [HuggingFace ðŸ¤—] [[Home Page](https://huggingface.co/docs/peft/index)] [[Github Repo](https://github.com/huggingface/peft)]<br>
PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. <br>

* **LLaMA Efficient Tuning** [[Github Repo](https://github.com/hiyouga/LLaMA-Efficient-Tuning)]<br>
Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA) (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen). <br>

* **LLaMA-Adapter: Efficient Fine-tuning of LLaMA** ðŸš€[[Github Repo](https://github.com/OpenGVLab/LLaMA-Adapter)]<br>
Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters <br>

* **LLaMA2-Accessory** ðŸš€[[Github Repo](https://github.com/Alpha-VLLM/LLaMA2-Accessory)]<br>
An Open-source Toolkit for LLM Development <be>

* **LLaMA Factory: Training and Evaluating Large Language Models with Minimal Effort** [Github Repo](https://github.com/hiyouga/LLaMA-Factory)]<br>
Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM3)
