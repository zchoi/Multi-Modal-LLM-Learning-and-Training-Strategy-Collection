# Multi-modal Large Language Model Collection ðŸ¦•
This is a curated list of Multi-modal Large Language Models (MLLM), Multimodal Benchmarks (MMB), Multimodal Instruction Tuning (MMIT), Multimodal In-context Learning (MMIL), Foundation Models (*e.g.*, CLIP families) (FM), and the most popular Parameter-Efficient Tuning methods.

## ðŸ“’Table of Contents
- [Multi-modal Large Language Models (MLLM)](#multimodal-large-language-models)
- [Multimodal Benchmarks (MMB)](#multimodal-benchmarks)
- [Foundation Models (FM)](#foundation-models)
- [Parameter-Efficient Tuning Repo (PETR)](#parameter-efficient-tuning-repo)

> ### Multi-modal Large Language Models (MLLM)

* **InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks** [Arxiv 2024/01/15] [[Paper](https://arxiv.org/abs/2312.14238)] [[Code](https://github.com/OpenGVLab/InternVL)]<br>
<sup>1</sup>OpenGVLab, Shanghai AI Laboratory <sup>2</sup>Nanjing University <sup>3</sup>The University of Hong Kong <sup>4</sup>The Chinese University of Hong Kong <sup>5</sup>Tsinghua University <sup>6</sup>University of Science and Technology of China <sup>7</sup>SenseTime Research

* **GiT: Towards Generalist Vision Transformer through Universal Language Interface** [Arxiv 2024/03/14] [[Paper](https://arxiv.org/abs/2403.09394)]<br>
<sup>1</sup>Peking University <sup>2</sup>Max Planck Institute for Informatics <sup>3</sup>The Chinese University of Hong Kong Shenzhen <sup>4</sup>ETH Zurich <sup>5</sup>The Chinese University of Hong Kong<br>

* **LLaMA: Open and Efficient Foundation Language Models** [Arxiv 2023] [[Paper](https://arxiv.org/pdf/2302.13971v1.pdf)] [[Github Repo](https://github.com/CHENGY12/PLOT)]<br>
<sup>1</sup>Meta AI


> ### Foundation Models (FM)

> ### Parameter-Efficient Tuning Repo (PETR)
* **PEFT: Parameter-Efficient Fine-Tuning** [HuggingFace ðŸ¤—] [[Home Page](https://huggingface.co/docs/peft/index)] [[Code](https://github.com/huggingface/peft)]<br>
PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. <br>

* **LLaMA Efficient Tuning** [[Github Repo](https://github.com/hiyouga/LLaMA-Efficient-Tuning)]<br>
Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA) (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen). <br>

* **LLaMA-Adapter: Efficient Fine-tuning of LLaMA** ðŸš€[[Code](https://github.com/OpenGVLab/LLaMA-Adapter)]<br>
Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters <br>

* **LLaMA2-Accessory** ðŸš€[[Code](https://github.com/Alpha-VLLM/LLaMA2-Accessory)]<br>
An Open-source Toolkit for LLM Development <be>

* **LLaMA Factory: Training and Evaluating Large Language Models with Minimal Effort** [Code](https://github.com/hiyouga/LLaMA-Factory)]<br>
Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM3)
